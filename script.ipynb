{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, open the coffee datasets, clean them and merge them into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def readXlsx(excel_file):\n",
    "    return pd.read_excel(excel_file)\n",
    "\n",
    "def clean(df):\n",
    "    df = df.iloc[2:,:]\n",
    "    df.columns = ['Date', 'Symbol', 'Warehouse', 'ProductionYear', 'Open', 'Close', 'High', 'Low', 'Change', 'PercentageChange', 'Volume']\n",
    "    return df\n",
    "\n",
    "df_cof = clean(readXlsx('Datasets/Coffee/Coffee-2012.xlsx'))\n",
    "df_cof_2013 = clean(readXlsx('Datasets/Coffee/Coffee-2013.xlsx'))\n",
    "df_cof_2014 = clean(readXlsx('Datasets/Coffee/Coffee-2014.xlsx'))\n",
    "df_cof_2015 = clean(readXlsx('Datasets/Coffee/Coffee-2015.xlsx'))\n",
    "df_cof_2016 = clean(readXlsx('Datasets/Coffee/Coffee-2016.xlsx'))\n",
    "df_cof_2017 = clean(readXlsx('Datasets/Coffee/Coffee-2017.xlsx'))\n",
    "df_cof_2018 = clean(readXlsx('Datasets/Coffee/Coffee-2018.xlsx'))\n",
    "df_cof_2019 = clean(readXlsx('Datasets/Coffee/Coffee-2019.xlsx'))\n",
    "\n",
    "df_cof = df_cof.append([df_cof_2013, df_cof_2014, df_cof_2015, df_cof_2016, df_cof_2017, df_cof_2018, df_cof_2019])\n",
    "df_cof.index = [i for i in range(1, df_cof.shape[0]+1)]\n",
    "df_cof.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Remove the timestamps from the Date column because they are useless. \n",
    "\n",
    "2. Discard the close column because all values are set to 0. \n",
    "\n",
    "3. Also remove all commas from the Open, High and Low columns.\n",
    "\n",
    "4. Data is not arranged chronologically, solve that.\n",
    "\n",
    "5. Divide the dataset based on the different symbols\n",
    "\n",
    "6. Since commodity prices don't move significantly within the span of a week, take the average of the opening prices for the \n",
    "   entire week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sr = df_cof.Symbol.value_counts(sort=True)\n",
    "sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequently traded coffee types are LUBP, WSDA and ULK types. We'll restrict our study to these types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df_cof_lubp = df_cof.loc[df_cof['Symbol'].str.contains('LUBP'), :]\n",
    "df_cof_wsda = df_cof.loc[df_cof['Symbol'].str.contains('WSDA'), :]\n",
    "df_cof_ulk = df_cof.loc[df_cof['Symbol'].str.contains('WYCA'), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen on the 'Symbol' column, there are different types of Coffee in the dataset. Identify those whose price-movements are correlated and use them to train the model with. Daily data is also troublesome to deal with. Resample the data frames into weekly format to ease work and improve performance. Since commodity price trends usually occur over a span of weeks and months, this should have little impact on the model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df_lubp_temp = df_cof_lubp.copy()\n",
    "df_wsda_temp = df_cof_wsda.copy()\n",
    "df_ulk_temp = df_cof_ulk.copy()\n",
    "\n",
    "df_lubp_temp['Date'] = pd.to_datetime(df_cof_lubp['Date'].dt.strftime('%Y/%m/%d'))\n",
    "df_lubp_temp.drop(['Close'], axis=1, inplace=True)\n",
    "df_wsda_temp['Date'] = pd.to_datetime(df_cof_wsda['Date'].dt.strftime('%Y/%m/%d'))\n",
    "df_wsda_temp.drop(['Close'], axis=1, inplace=True)\n",
    "df_ulk_temp['Date'] = pd.to_datetime(df_ulk_temp['Date'].dt.strftime('%Y/%m/%d'))\n",
    "df_ulk_temp.drop(['Close'], axis=1, inplace=True)\n",
    "\n",
    "def toNumeric(col):\n",
    "    str_col = [str(x) for x in col]\n",
    "    float_col = []\n",
    "    for x in str_col:\n",
    "        if x.find(',') != -1:\n",
    "            var = x[0:x.find(',')] + x[x.find(',')+1:]\n",
    "            float_col.append(float(var))\n",
    "        else:\n",
    "            float_col.append(x)\n",
    "    return pd.to_numeric(float_col)\n",
    "\n",
    "df_lubp_temp['Open'] = toNumeric(df_lubp_temp.Open)\n",
    "df_lubp_temp['Volume'] = toNumeric(df_lubp_temp.Volume)\n",
    "df_wsda_temp['Open'] = toNumeric(df_wsda_temp.Open)\n",
    "df_wsda_temp['Volume'] = toNumeric(df_wsda_temp.Volume)\n",
    "df_ulk_temp['Open'] = toNumeric(df_ulk_temp.Open)\n",
    "df_ulk_temp['Volume'] = toNumeric(df_ulk_temp.Volume)\n",
    "\n",
    "df_lubp_temp = df_lubp_temp.sort_values(by='Date')\n",
    "df_wsda_temp = df_wsda_temp.sort_values(by='Date')\n",
    "df_ulk_temp = df_ulk_temp.sort_values(by='Date')\n",
    "df_lubp_temp = df_lubp_temp.set_index('Date')\n",
    "df_wsda_temp = df_wsda_temp.set_index('Date')\n",
    "df_ulk_temp = df_ulk_temp.set_index('Date')\n",
    "\n",
    "df_cof_lubp = pd.DataFrame(df_lubp_temp['Open'].resample('W').mean())\n",
    "df_cof_lubp['Symbol'] = df_lubp_temp['Symbol'].resample('W').first()\n",
    "df_cof_lubp['Volume'] = df_lubp_temp['Volume'].resample('W').mean()\n",
    "\n",
    "df_cof_wsda = pd.DataFrame(df_wsda_temp['Open'].resample('W').mean())\n",
    "df_cof_wsda['Symbol'] = df_wsda_temp['Symbol'].resample('W').first()\n",
    "df_cof_wsda['Volume'] = df_wsda_temp['Volume'].resample('W').mean()\n",
    "\n",
    "df_cof_ulk = pd.DataFrame(df_ulk_temp['Open'].resample('W').mean())\n",
    "df_cof_ulk['Symbol'] = df_ulk_temp['Symbol'].resample('W').first()\n",
    "df_cof_ulk['Volume'] = df_ulk_temp['Volume'].resample('W').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a chart depicting the correlation between the LUBP coffee standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "def plotDataFrame(data_frames, color):\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    for df, c in zip(data_frames, color):\n",
    "        row = df.iloc[1:2, :]\n",
    "        val = str(row['Symbol'].values)\n",
    "        label = val[2:-2]\n",
    "        plt.plot(df.index, df['Open'], c=c, label=label)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def separateTypes(df_cof_type):\n",
    "    sr = df_cof_type.Symbol.value_counts(sort=True)\n",
    "    print(len(sr))\n",
    "    data_frames_type = []\n",
    "    for item in sr.iteritems():\n",
    "        df_item = df_cof_type.loc[df_cof_type['Symbol'] == item[0], :]\n",
    "        df_item.Open = pd.to_numeric(df_item.Open)\n",
    "        data_frames_type.append(df_item)\n",
    "    return data_frames_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames_lubp = separateTypes(df_cof_lubp)\n",
    "count = 0\n",
    "color = cm.rainbow(np.linspace(0, 1, len(data_frames_lubp)))\n",
    "\n",
    "plotDataFrame(data_frames_lubp, color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Below is a chart depicting the correlation between the WSDA coffee standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_frames_wsda = separateTypes(df_cof_wsda)\n",
    "count = 0\n",
    "color = cm.rainbow(np.linspace(0, 1, len(data_frames_wsda)))\n",
    "\n",
    "plotDataFrame(data_frames_wsda, color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a chart depicting the correlation between the ULK coffee standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_frames_ulk = separateTypes(df_cof_ulk)\n",
    "\n",
    "count = 0\n",
    "color = cm.rainbow(np.linspace(0, 1, len(data_frames_ulk)))\n",
    "\n",
    "plotDataFrame(data_frames_ulk, color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a chart depicting the correlation between the Total Volume of the three coffee standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "volume_lubp = df_cof_lubp.Volume.sum()\n",
    "volume_ulk = df_cof_ulk.Volume.sum()\n",
    "volume_wsda = df_cof_wsda.Volume.sum()\n",
    "\n",
    "volume_frames = [volume_lubp, volume_ulk, volume_wsda]\n",
    "volume_ticks = ('LUBP', 'ULK', 'WSDA')\n",
    "\n",
    "x = [0, 1, 2]\n",
    "fig, ax = plt.subplots() \n",
    "plt.bar(x, volume_frames)\n",
    "plt.xticks(x, volume_ticks)\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey='row', figsize=(20, 7))\n",
    "\n",
    "lag_plot(df_cof_lubp['Open'], ax=ax1)\n",
    "lag_plot(df_cof_wsda['Open'], ax=ax2)\n",
    "lag_plot(df_cof_ulk['Open'], ax=ax3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonal Decompositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken individually, the coffee standard types are sparsely traded. But since we know their prices to be almost equal and\n",
    "proportionally traded, we will use the integrated dataframe to find hidden patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "y_lubp = df_cof_lubp['Open']\n",
    "y1 = y_lubp.loc[y_lubp.index < '2019-01-01']\n",
    "decompose = sm.tsa.seasonal_decompose(y1, model='additive')\n",
    "fig = decompose.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WSDA and ULK coffee types have missing values so deal with them before performing seasonal decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cof_wsda = df_cof_wsda.fillna(method='pad')\n",
    "y_wsda = df_cof_wsda['Open']\n",
    "y2 = y_wsda.loc[y_wsda.index < '2019-01-01']\n",
    "decompose = sm.tsa.seasonal_decompose(y2, model='additive')\n",
    "fig = decompose.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cof_ulk = df_cof_ulk.fillna(method='pad')\n",
    "y_ulk = df_cof_ulk['Open']\n",
    "y3 = y_ulk.loc[y_ulk.index < '2019-01-01']\n",
    "decompose = sm.tsa.seasonal_decompose(y3, model='additive')\n",
    "fig = decompose.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Grid Search to find the optimal non-seasonal and seasonal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "\n",
    "p, d, q = range(0, 5), range(0, 3), range(0, 3), \n",
    "sp, sd, sq = range(0, 5), range(0, 3), range(0, 2)\n",
    "pdq, s_pdq = list(itertools.product(p, d, q)), list(itertools.product(sp, sd, sq, ))\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 52) for x in s_pdq]\n",
    "lowest_aic = 10000\n",
    "optimal, optimal_seasonal = (0, 0, 0), (0, 0, 0)\n",
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        mod = sm.tsa.statespace.SARIMAX(y1,\n",
    "                                       order=param,\n",
    "                                       seasonal_order=param_seasonal,\n",
    "                                       enforce_stationarity=False,\n",
    "                                       enforce_invertibility=False)\n",
    "        results = mod.fit()\n",
    "        if results.aic < lowest_aic:\n",
    "            lowest_aic = results.aic\n",
    "            optimal = param\n",
    "            optimal_seasonal = param_seasonal\n",
    "            \n",
    "        print('{}x{}-{}'.format(param, param_seasonal, results.aic))\n",
    "print('lowest AIC = {}, params = {}, seasonal params = {}'.format(lowest_aic, optimal, optimal_seasonal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.tsa.statespace.SARIMAX(y1,\n",
    "                               order=(3, 2, 1),\n",
    "                               seasonal_order = (4, 1, 1, 52),\n",
    "                               enforce_stationarity=False,\n",
    "                               enforce_invertibility=False)\n",
    "result_lubp = mod.fit()\n",
    "print(result_lubp.aic)\n",
    "result_lubp.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above suggests that SARIMAX(4, 2, 2)x(4, 2, 1, 52) yields the lowest AIC value of 568.74. Therefore we should take that as the optimal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lubp.plot_diagnostics(figsize=(20, 7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = d = q = range(0, 5)\n",
    "sp, sd, sq = range(0, 5), range(0, 3), range(0, 2)\n",
    "pdq = itertools.product(p, d, q)\n",
    "seasonal_pdq = itertools.product(sp, sd, sq, range(52, 53))\n",
    "lowest_aic = 10000\n",
    "optimal, optimal_seasonal = (0, 0, 0), (0, 0, 0)\n",
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        mod = sm.tsa.statespace.SARIMAX(y2,\n",
    "                                       order=param,\n",
    "                                       seasonal_order=param_seasonal,\n",
    "                                       enforce_stationarity=False,\n",
    "                                       enforce_invertibility=False)\n",
    "        results_wsda = mod.fit()\n",
    "        \n",
    "        if results_wsda.aic < lowest_aic:\n",
    "            lowest_aic = results_wsda.aic\n",
    "            optimal = param\n",
    "            optimal_seasonal = param_seasonal\n",
    "            \n",
    "print('lowest AIC = {}, params = {}, seasonal params = {}'.format(lowest_aic, optimal, optimal_seasonal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.tsa.statespace.SARIMAX(y2,\n",
    "                                 order=(4, 2, 2),\n",
    "                                 seasonal_order=(4, 2, 1, 52),\n",
    "                                 enforce_stationarity=False,\n",
    "                                 enforce_invertibility=False)\n",
    "results_wsda = model.fit()\n",
    "print(results_wsda.aic)\n",
    "results_wsda.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = results_wsda.plot_diagnostics(figsize=(20, 7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = range(0, 5)\n",
    "d = range(0, 3)\n",
    "q = range(0, 3)\n",
    "m = range(52, 53)\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "seasonal_pdq = (itertools.product(p, d, range(0, 2), m))\n",
    "lowest_aic = 10000\n",
    "optimal, optimal_seasonal = (0, 0, 0), (0, 0, 0)\n",
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        mod = sm.tsa.statespace.SARIMAX(y3,\n",
    "                                       order=param,\n",
    "                                       seasonal_order=param_seasonal,\n",
    "                                       enforce_stationarity=False,\n",
    "                                       enforce_invertibility=False)\n",
    "        results_ulk = mod.fit()\n",
    "        \n",
    "        if results_ulk.aic < lowest_aic:\n",
    "            lowest_aic = results_ulk.aic\n",
    "            optimal = param\n",
    "            optimal_seasonal = param_seasonal\n",
    "        \n",
    "print('lowest AIC = {}, params = {}, seasonal params = {}'.format(lowest_aic, optimal, optimal_seasonal))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.tsa.statespace.SARIMAX(y3,\n",
    "                               order=(3, 1, 1),\n",
    "                               seasonal_order=(4, 2, 1, 52),\n",
    "                               enforce_stationarity=False,\n",
    "                               enforce_invertibility=False)\n",
    "results_ulk = mod.fit()\n",
    "print(results_ulk.aic)\n",
    "results_ulk.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ulk = results_ulk.get_prediction(start=pd.to_datetime('2019-01-06'), end=pd.to_datetime('2019-09-15'), dynamic=False)\n",
    "ax = y_ulk['2019':].plot(label='WYCA observed')\n",
    "pred_ulk.predicted_mean.plot(ax=ax, label='One-step ahead forecast', alpha=0.7, figsize=(20, 7))\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lubp = result_lubp.get_prediction(start=pd.to_datetime('2019-01-06'), end=pd.to_datetime('2019-09-15'), dynamic=False)\n",
    "pred_lubp_ci = pred_lubp.conf_int()\n",
    "\n",
    "ax = y_lubp['2019':].plot(label='LUBP observed')\n",
    "pred_lubp.predicted_mean.plot(ax=ax, label='One-step ahead forecast', alpha=0.8, figsize=(20, 7))\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_wsda = results_wsda.get_prediction(start=pd.to_datetime('2019-01-06'), end=pd.to_datetime('2019-09-15'), dynamic=False)\n",
    "pred_wsda_ci = pred_wsda.conf_int()\n",
    "\n",
    "ax = y_wsda['2019':].plot(label='WSDA observed')\n",
    "pred_wsda.predicted_mean.plot(ax=ax, label='One-step ahead forecast', alpha=0.7, figsize=(20, 7))\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ulk = results_ulk.get_prediction(start=pd.to_datetime('2019-01-06'), end=pd.to_datetime('2019-09-15'), dynamic=False)\n",
    "pred_ci_ulk = pred_ulk.conf_int()\n",
    "ax = y_ulk['2019':].plot(label='WYCA observed')\n",
    "pred_ulk.predicted_mean.plot(ax=ax, label='One-step ahead forecast', alpha=0.7, figsize=(20, 7))\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the Mean Absolute Percentage Errors (MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list(pred_lubp.predicted_mean)\n",
    "response = list(y_lubp['2019':])\n",
    "\n",
    "mape = 0\n",
    "for pred, resp in zip(predictions, response):\n",
    "    mape += abs(pred - resp) / resp * 100\n",
    "mape /= len(predictions)\n",
    "mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_wsda = list(pred_wsda.predicted_mean)\n",
    "response_wsda = list(y_wsda['2019':])\n",
    "\n",
    "mape_wsda = 0\n",
    "for pred, resp in zip(predictions_wsda, response_wsda):\n",
    "    mape_wsda += abs(pred - resp) / resp * 100\n",
    "mape_wsda /= len(predictions_wsda)\n",
    "mape_wsda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_wyca = list(pred_ulk.predicted_mean)\n",
    "response_wyca = list(y_ulk['2019':])\n",
    "\n",
    "mape_wyca = 0\n",
    "for pred, resp in zip(predictions_wyca, response_wyca):\n",
    "    mape_wyca += abs(pred - resp) / resp * 100\n",
    "mape_wyca /= len(predictions_wyca)\n",
    "mape_wyca"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
